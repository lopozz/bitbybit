{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af0526a",
   "metadata": {},
   "source": [
    "The reconstruction from the *prorotype* samples in tha latent created by the Convolution-based autoencoder was not succcessfull. Even if the model is able to have recreate images in a descrete way from th etest set, the latent space is still to sparse and the result is that there are large pockets of white space between cluters. In these pockets, the model cannot be expected to produce any reasonable generation, as the data has not been mapped there.\n",
    "\n",
    "This is because we have absolutely no regularization on our latent space, it is allowed to be whatever it needs to be to minimize our MSE Reconstruction Loss. If we are going to be sampling from a distribution, we have to pick which one we want, and the simplicity of normal distributions make them a natural choice. Therefore we will force our model to learn \n",
    "$P(x)$ by forcing the reconstruction loss AND making sure to map our latents to a nice Standard Normal distribution rather an unrestricted arbritrary one. If our latents are normally distributed, we know exactly the properties of it and our life becomes easier when we want to use it for some generative tasks.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.researchgate.net/publication/346701232/figure/fig2/AS:969513902292992@1608161535014/Variational-Autoencoder-VAE-VAEs-40-are-used-to-learn-representations-of-the.png\" alt=\"alt\" width=\"700\">\n",
    "</p>\n",
    "\n",
    "So we have set the goal. What we want is a model that can perform the reconstruction task like before, with the added restriction that the latent space must be close to a normal distribution. How Do We Force Latents To Be Gaussian?\n",
    "\n",
    "There are two things we have to do:\n",
    "\n",
    "- **Reconstruction Loss**: Make sure the output to the model looks like the input to the model, this is already done\n",
    "- **KL Divergence Loss**: Measure the distance of our latent distribution from a Standard Normal and minimize it\n",
    "\n",
    "\n",
    "### Kullback-Leibler (KL) Divergence\n",
    "\n",
    "KL Divergence is a measure of entropy (or difference) between two probability distributions.\n",
    "To understand it properly, we first need to understand entropy and cross-entropy.\n",
    "\n",
    "Entropy measures how uncertain or unpredictable a probability distribution $P$ is. It is typically written as:\n",
    "\n",
    "$$\n",
    "H(P) = - \\sum_{i=1}^{N} p(x_i) \\cdot \\log p(x_i)\n",
    "$$\n",
    "\n",
    "- A **low entropy** distribution means the outcomes are very predictable.\n",
    "  Most of the probability mass is concentrated on a few outcomes, so observing\n",
    "  the data is not very surprising.\n",
    "\n",
    "- A **high entropy** distribution means the outcomes are very unpredictable.\n",
    "  Probability mass is spread out, so each observation carries more surprise\n",
    "  on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb096622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-entropy distribution: [0.97, 0.01, 0.01, 0.01]\n",
      "Entropy: 0.16770053683981007\n",
      "\n",
      "High-entropy distribution: [0.25, 0.25, 0.25, 0.25]\n",
      "Entropy: 1.3862943611198906\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(p):\n",
    "    \"\"\"Compute entropy H(P) = -sum p log p\"\"\"\n",
    "    p = np.array(p)\n",
    "    p = p[p > 0]  # avoid log(0)\n",
    "    return -np.sum(p * np.log(p))\n",
    "\n",
    "\n",
    "# Low-entropy distribution (very predictable)\n",
    "p_low = [0.97, 0.01, 0.01, 0.01] # [1.0 , 0.0] is a deterministic distribution\n",
    "\n",
    "# High-entropy distribution (uniform, maximally uncertain)\n",
    "p_high = [0.25, 0.25, 0.25, 0.25]\n",
    "\n",
    "print(\"Low-entropy distribution:\", p_low)\n",
    "print(\"Entropy:\", entropy(p_low))\n",
    "\n",
    "print(\"\\nHigh-entropy distribution:\", p_high)\n",
    "print(\"Entropy:\", entropy(p_high))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b9f478",
   "metadata": {},
   "source": [
    "Entropy assumes that we use the *correct* distribution to measure surprise.\n",
    "However, in practice, we often do not know the true distribution that generated\n",
    "the data.\n",
    "\n",
    "Now suppose the data is actually generated by a true distribution $P$, but we\n",
    "describe, encode, or model the data using a different distribution $Q$.\n",
    "In this case, the surprise of observing an outcome $x_i$ is not measured by\n",
    "$-\\log p(x_i)$ (which assumes perfect knowledge), but by $-\\log q(x_i)$,\n",
    "because $Q$ is the model we are using.\n",
    "\n",
    "A **low cross-entropy** means that $Q$ assigns high probability to outcomes\n",
    "that are likely under $P$, so the model matches the data well.\n",
    "\n",
    "A **high cross-entropy** means that $Q$ assigns low probability to outcomes\n",
    "that frequently occur under $P$, leading to large surprise and inefficient\n",
    "encoding.\n",
    "\n",
    "This expected surprise when data from $P$ is measured using $Q$ is called\n",
    "the **cross-entropy** between $P$ and $Q$:\n",
    "\n",
    "$$\n",
    "H(P, Q) = - \\sum_{i=1}^{N} p(x_i) \\cdot \\log q(x_i)\n",
    "$$\n",
    "\n",
    "Cross-entropy alone is **not** a pure measure of how different two distributions\n",
    "are. This is because it mixes two effects:\n",
    "1. The intrinsic uncertainty of the true distribution $P$\n",
    "2. The mismatch between the model distribution $Q$ and the true distribution $P$\n",
    "\n",
    "Even if the model is perfect ($Q = P$), the cross-entropy does not vanish:\n",
    "\n",
    "$$\n",
    "H(P, P) = H(P)\n",
    "$$\n",
    "\n",
    "This means cross-entropy can be large simply because the data distribution itself\n",
    "has high entropy, not because the model is inaccurate. To isolate *only* the difference between distributions, we subtract the entropy of the true distribution. This leads to the definition of **KL Divergence**. [KL Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (also called **relative entropy**) measures how much an approximating probability distribution $Q$ is different from a true probability distribution $P$. It is defined as the difference between cross-entropy and entropy:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\parallel Q) = H(P, Q) - H(P)\n",
    "$$\n",
    "\n",
    "Intuitively KL divergence subtracts out the part that **cannot improve** and isolate the distance between real and modeled distribution. Substituting the definitions of entropy and cross-entropy gives:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{KL}(P \\parallel Q)\n",
    "&= - \\sum_{i=1}^{N} p(x_i) \\log q(x_i)\n",
    "  + \\sum_{i=1}^{N} p(x_i) \\log p(x_i) \\\\\n",
    "&= \\sum_{i=1}^{N} p(x_i) \\cdot (\\log p(x_i) - \\log q(x_i))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}_{\\theta, \\phi}(x; \\epsilon)\n",
    "=\n",
    "\\underbrace{\\log p_{\\theta}(x \\mid z)}_{\\text{Negative reconstruction error}}\n",
    "+\n",
    "\\underbrace{\\log p_{\\theta}(z) - \\log q_{\\phi}(z \\mid x)}_{\\text{Regularization terms}}\n",
    "$$\n",
    "\n",
    "Using the logarithm identity $\\log a - \\log b = \\log \\frac{a}{b}$, this can be written as:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\parallel Q)\n",
    "= \\sum_{i=1}^{N} p(x_i) \\cdot \\log \\frac{p(x_i)}{q(x_i)}\n",
    "$$\n",
    "\n",
    "KL Divergence is always non-negative and equals zero if and only if $P = Q$. Our goal is to minimize this value.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In Bayesian Statistics, we typically have this form:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Each piece of this actually has a name!\n",
    "\n",
    "- $P(A \\mid B)$: **Posterior Probability**, the probability of A occurring knowing B had occurred\n",
    "- $P(B \\mid A)$: **Likelihood Function**, the probability of B occurring knowing A had occurred\n",
    "- $P(A)$ : **Prior Probability**, some initial belief you had about A before any evidence from data is utilized\n",
    "- $P(B)$ : **Marginal (Evidence) Probability**, summing across all possible A in the joint distribution $P(A, B)$\n",
    "\n",
    "In our case, we will use $P(z \\mid x)$  where $z$  is our latent (compressed) variables and $x$  is the data.\n",
    "This means we can write our encoder as $P(z \\mid x)$  which is just our posterior, and the decoder $P(x \\mid z)$  which is our likelihood function.\n",
    "\n",
    "Let’s update our notation a bit so it follows what you typically see online.\n",
    "\n",
    "- $q_\\theta(z_i \\mid x)$: The distribution of our latent variables $z_i$ given $x$,\n",
    "  approximated by a neural network with parameters $\\theta$.\n",
    "\n",
    "- $p(z_i \\mid x)$: The simpler distribution we selected to approximate with\n",
    "  (typically a standard multivariate normal).\n",
    "\n",
    "Our goal can then be written as:\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\; D_{KL}\\big(q_\\theta(z_i \\mid x) \\,\\|\\, p(z_i \\mid x)\\big)\n",
    "$$\n",
    "\n",
    "which expands to:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} q_\\theta(z_i \\mid x) \\cdot\n",
    "\\log \\frac{q_\\theta(z_i \\mid x)}{p(z_i \\mid x)}\n",
    "$$\n",
    "\n",
    "We can then use the conditional probability rule and rewrite:\n",
    "\n",
    "$$\n",
    "p(z_i \\mid x) = \\frac{p(z_i, x)}{p(x)}\n",
    "$$\n",
    "\n",
    "So we then substitute into our KL Divergence:\n",
    "\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} q_\\theta(z_i \\mid x) \\cdot\n",
    "\\log \\frac{q_\\theta(z_i \\mid x)\\, p(x)}{p(z_i, x)}\n",
    "$$\n",
    "\n",
    "Applying the logarithm rule $\\log(ab) = \\log a + \\log b$ gives:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} q_\\theta(z_i \\mid x) \\cdot\n",
    "\\log \\frac{q_\\theta(z_i \\mid x)}{p(z_i, x)}\n",
    "\\;+\\;\n",
    "\\sum_{i=1}^{N} q_\\theta(z_i \\mid x) \\cdot \\log p(x)\n",
    "$$\n",
    "\n",
    "In the second sum, $\\log p(x)$ is a constant and does not depend on $z$,\n",
    "so we can factor it out:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} q_\\theta(z_i \\mid x) \\cdot\n",
    "\\log \\frac{q_\\theta(z_i \\mid x)}{p(z_i, x)}\n",
    "\\;+\\;\n",
    "\\log p(x) \\cdot \\sum_{i=1}^{N} q_\\theta(z_i \\mid x)\n",
    "$$\n",
    "\n",
    "The second summation is over a probability distribution across all possible\n",
    "latent values $z_i$, which must sum to 1:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} q_\\theta(z_i \\mid x) = 1\n",
    "$$\n",
    "\n",
    "We are therefore left with:\n",
    "\n",
    "$$\n",
    "D_{KL}\\big(q_\\theta(z_i \\mid x)\\,\\|\\,p(z_i \\mid x)\\big)\n",
    "=\n",
    "\\sum_{i=1}^{N} q_\\theta(z_i \\mid x) \\cdot\n",
    "\\log \\frac{q_\\theta(z_i \\mid x)}{p(z_i, x)}\n",
    "\\;+\\;\n",
    "\\log p(x)\n",
    "$$\n",
    "\n",
    "Finally, using the relationship between joint and conditional distributions,\n",
    "we can write:\n",
    "\n",
    "$$\n",
    "p(z_i, x) = p(z_i \\mid x)p(x) = p(x \\mid z_i)p(z_i)\n",
    "$$\n",
    "\n",
    "In this case, we definitely prefer the second form\n",
    "$p(x \\mid z_i)p(z_i)$ as everything in there is computable.\n",
    "There are no nasty integrals over the unknown latent space $z$.\n",
    "The first form contains $p(x)$, which would again require an\n",
    "intractable integral. So let’s substitute this in:\n",
    "\n",
    "$$\n",
    "D_{KL}\\big(q_\\theta(z_i \\mid x)\\,\\|\\,p(z_i \\mid x)\\big)\n",
    "=\n",
    "\\left(\n",
    "\\sum_{i=1}^{N}\n",
    "q_\\theta(z_i \\mid x)\\cdot\n",
    "\\log \\frac{q_\\theta(z_i \\mid x)}{p(x \\mid z_i)p(z_i)}\n",
    "\\right)\n",
    "+\n",
    "\\log p(x)\n",
    "$$\n",
    "\n",
    "Let’s rearrange the terms a bit:\n",
    "\n",
    "$$\n",
    "-\n",
    "\\left(\n",
    "\\sum_{i=1}^{N}\n",
    "q_\\theta(z_i \\mid x)\\cdot\n",
    "\\log \\frac{q_\\theta(z_i \\mid x)}{p(x \\mid z_i)p(z_i)}\n",
    "\\right)\n",
    "=\n",
    "\\log p(x)\n",
    "-\n",
    "D_{KL}\\big(q_\\theta(z_i \\mid x)\\,\\|\\,p(z_i \\mid x)\\big)\n",
    "$$\n",
    "\n",
    "We can remove the negative sign on the left by flipping the fraction\n",
    "inside the logarithm:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N}\n",
    "q_\\theta(z_i \\mid x)\\cdot\n",
    "\\log \\frac{p(x \\mid z_i)p(z_i)}{q_\\theta(z_i \\mid x)}\n",
    "=\n",
    "\\log p(x)\n",
    "-\n",
    "D_{KL}\\big(q_\\theta(z_i \\mid x)\\,\\|\\,p(z_i \\mid x)\\big)\n",
    "$$\n",
    "\n",
    "A key property of KL divergence is that it is never less than zero:\n",
    "$$\n",
    "D_{KL}(\\cdot \\| \\cdot) \\ge 0\n",
    "$$\n",
    "\n",
    "For clarity, let us rewrite the equation as:\n",
    "$$\n",
    "A = B - C\n",
    "$$\n",
    "\n",
    "From this, we can conclude two important things:\n",
    "\n",
    "- If we want to minimize $C$ (our placeholder for KL divergence),\n",
    "  this is equivalent to maximizing $A$. Therefore, our optimization\n",
    "  problem changes from\n",
    "\n",
    "$$\n",
    "\\min_\\theta\n",
    "D_{KL}\\big(q_\\theta(z_i \\mid x)\\,\\|\\,p(z_i \\mid x)\\big)\n",
    "$$\n",
    "\n",
    "to\n",
    "\n",
    "$$\n",
    "\\max_\\theta\n",
    "\\sum_{i=1}^{N}\n",
    "q_\\theta(z_i \\mid x)\\cdot\n",
    "\\log \\frac{p(x \\mid z_i)p(z_i)}{q_\\theta(z_i \\mid x)}\n",
    "$$\n",
    "\n",
    "- Since $C \\ge 0$ due to the properties of KL divergence,\n",
    "  it follows that $A \\le B$. This means the left-hand side is always\n",
    "  a lower bound on $\\log p(x)$.\n",
    "\n",
    "\n",
    "Using these properties, let’s rewrite the expression:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N}\n",
    "q_\\theta(z_i \\mid x)\\cdot\n",
    "\\log \\frac{p(x \\mid z_i)p(z_i)}{q_\\theta(z_i \\mid x)}\n",
    "\\;\\le\\;\n",
    "\\log p(x)\n",
    "$$\n",
    "\n",
    "Let’s take a look at the two conditional probabilities we have:\n",
    "\n",
    "- $q_\\theta(z_i \\mid x)$:\n",
    "  Use a neural network parameterized by $\\theta$ to predict the latent variable $z_i$.\n",
    "  This is handled by the **encoder** and represents our **posterior distribution**.\n",
    "\n",
    "- $p(x \\mid z_i)$:\n",
    "  This is the reverse process and corresponds to the **likelihood function**.\n",
    "  It is learned jointly by our **decoder**, which we parameterize by $\\phi$.\n",
    "\n",
    "Therefore, our encoder is $q_\\theta(z_i \\mid x)$ and our decoder is\n",
    "$p_\\phi(x \\mid z_i)$. Substituting this into the inequality gives:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N}\n",
    "q_\\theta(z_i \\mid x)\\cdot\n",
    "\\log \\frac{p_\\phi(x \\mid z_i)p(z_i)}{q_\\theta(z_i \\mid x)}\n",
    "\\;\\le\\;\n",
    "\\log p(x)\n",
    "$$\n",
    "\n",
    "Our optimization goal is to maximize the left-hand side of this equation,\n",
    "which is a lower bound on $\\log p(x)$. For this reason, it is known as the\n",
    "**Evidence Lower Bound (ELBO)** of the original data distribution.\n",
    "\n",
    "As we maximize this objective, we are equivalently maximizing the likelihood\n",
    "of the data distribution as well.\n",
    "\n",
    "Therefore, our final optimization problem becomes:\n",
    "\n",
    "\n",
    "$$\n",
    "\\min_{\\theta}\\;\n",
    "\\sum_{i=1}^{N}\n",
    "p(x_i)\\cdot\n",
    "\\log \\frac{p(x_i)}{q_\\theta(x_i)}\n",
    "\\;\\;\\Longrightarrow\\;\\;\n",
    "\\max_{\\theta,\\phi}\\;\n",
    "\\sum_{i=1}^{N}\n",
    "q_\\theta(z_i \\mid x)\\cdot\n",
    "\\log \\frac{p_\\phi(x \\mid z_i)p(z_i)}{q_\\theta(z_i \\mid x)}\n",
    "$$\n",
    "\n",
    "now it is expressed in function of the encoder and decoder which will be optimized using backprop.\n",
    "\n",
    "---\n",
    "\n",
    "### Imposing Standard Normal Distributions on our Latents\n",
    "Remember, it is our decision what distribution we want to use for these latents, and typically we use Normal Distributions! Lets write everything as Univariate Normal Distributions for now to keep the notation shorter, but the derivation will expand to Multivariate gaussians pretty easily (left as exercise).\n",
    "\n",
    "Quick review of Gaussians.\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}\n",
    "\\; e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2}\n",
    "$$\n",
    "\n",
    "\n",
    "A Gaussian distribution is defined (parameterized) by $\\mu$ and $\\sigma$.\n",
    "A 1D Gaussian has a single mean $\\mu$ and a single standard deviation $\\sigma$,\n",
    "and we can generate single numbers from this distribution that are normally\n",
    "distributed.\n",
    "\n",
    "[link](https://en.wikipedia.org/wiki/File:Galton_box.webm)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.shutterstock.com/shutterstock/videos/1110291311/thumb/6.jpg?ip=x480\" alt=\"alt\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "A Gaussian distribution with $\\mu = 0$ and $\\sigma = 1$ is known as\n",
    "**Standard Normal distribution**.\n",
    "\n",
    "The problem is that if we took data and compressed it down to a latent space\n",
    "with a dimension of just a single number, we would likely not achieve very good\n",
    "performance. Instead, we want to compress the data down to a *vector* of numbers.\n",
    "This vector should be normally distributed and sampled from a **Standard Multivariate Gaussian distribution**.\n",
    "\n",
    "A multivariate Gaussian distribution is defined by:\n",
    "\n",
    "- A mean vector $\\vec{\\mu}_d$ that defines the center of the distribution in\n",
    "  $d$-dimensional space\n",
    "- A covariance matrix $\\Sigma_{d \\times d}$ that specifies the variance of each\n",
    "  variable and the covariance between each pair of variables\n",
    "- A sampled data vector $\\vec{x}_d$\n",
    "\n",
    "More specifically, these quantities are defined as:\n",
    "\n",
    "$$\n",
    "\\vec{x}_d = [x_1, x_2, \\ldots, x_d]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{\\mu}_d =\n",
    "[\\mu_1 = \\text{mean}(x_1),\\;\n",
    " \\mu_2 = \\text{mean}(x_2),\\;\n",
    " \\ldots,\\;\n",
    " \\mu_d = \\text{mean}(x_d)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_{d \\times d} =\n",
    "\\begin{bmatrix}\n",
    "\\text{var}(x_1) & \\text{cov}(x_1, x_2) & \\cdots & \\text{cov}(x_1, x_d) \\\\\n",
    "\\text{cov}(x_2, x_1) & \\text{var}(x_2) & \\cdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{cov}(x_1, x_d) & \\cdots & \\cdots & \\text{var}(x_d)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To further simplify this, for a **Standard Normal Multivariate Gaussian** distribution, the mean vector and covariance matrix take a very simple form: $\\vec{\\mu}_d = \\vec{0}$ and $\\Sigma_{d \\times d} = I$ where $I$ is the identity matrix.\n",
    "\n",
    "Back to the derivation of the loss. \n",
    "\n",
    "Normal distributions are represented by a mean $\\mu$ and variance $\\sigma^2$.\n",
    "Let us allow $q_\\theta(z \\mid x)$ to have mean $\\mu_q$ and variance $\\sigma_q^2$,\n",
    "and allow $p(z)$ to have mean $\\mu_p$ and variance $\\sigma_p^2$.\n",
    "Then we can write:\n",
    "\n",
    "$$\n",
    "q_\\theta(z \\mid x)\n",
    "=\n",
    "\\frac{1}{\\sigma_q \\sqrt{2\\pi}}\n",
    "\\; e^{\n",
    "-\\frac{1}{2}\n",
    "\\left(\\frac{x - \\mu_q}{\\sigma_q}\\right)^2\n",
    "}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(z)\n",
    "=\n",
    "\\frac{1}{\\sigma_p \\sqrt{2\\pi}}\n",
    "\\; e^{\n",
    "-\\frac{1}{2}\n",
    "\\left(\\frac{x - \\mu_p}{\\sigma_p}\\right)^2\n",
    "}\n",
    "$$\n",
    "\n",
    "Let us now plug these expressions into the KL divergence formula and simplify:\n",
    "\n",
    "$$\n",
    "D_{KL}\\big(q_\\theta(z \\mid x)\\,\\|\\,p(z)\\big)\n",
    "=\n",
    "\\sum_{z}\n",
    "q_\\theta(z \\mid x)\\cdot\n",
    "\\log \\frac{q_\\theta(z \\mid x)}{p(z)}\n",
    "$$\n",
    "\n",
    "Substituting the Gaussian densities gives:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\sum_{z}\n",
    "\\frac{1}{\\sigma_q \\sqrt{2\\pi}}\n",
    "\\; e^{\n",
    "-\\frac{1}{2}\n",
    "\\left(\\frac{x - \\mu_q}{\\sigma_q}\\right)^2\n",
    "}\n",
    "\\cdot\n",
    "\\log\n",
    "\\frac{\n",
    "\\frac{1}{\\sigma_q \\sqrt{2\\pi}}\n",
    "\\; e^{\n",
    "-\\frac{1}{2}\n",
    "\\left(\\frac{x - \\mu_q}{\\sigma_q}\\right)^2\n",
    "}\n",
    "}{\n",
    "\\frac{1}{\\sigma_p \\sqrt{2\\pi}}\n",
    "\\; e^{\n",
    "-\\frac{1}{2}\n",
    "\\left(\\frac{x - \\mu_p}{\\sigma_p}\\right)^2\n",
    "}\n",
    "}\n",
    "$$\n",
    "\n",
    "Simplifying constants inside the logarithm:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\sum_{z}\n",
    "\\frac{1}{\\sigma_q \\sqrt{2\\pi}}\n",
    "\\; e^{\n",
    "-\\frac{1}{2}\n",
    "\\left(\\frac{x - \\mu_q}{\\sigma_q}\\right)^2\n",
    "}\n",
    "\\cdot\n",
    "\\log\n",
    "\\frac{\n",
    "\\sigma_p\n",
    "\\; e^{\n",
    "-\\frac{1}{2}\n",
    "\\left(\\frac{x - \\mu_q}{\\sigma_q}\\right)^2\n",
    "}\n",
    "}{\n",
    "\\sigma_q\n",
    "\\; e^{\n",
    "-\\frac{1}{2}\n",
    "\\left(\\frac{x - \\mu_p}{\\sigma_p}\\right)^2\n",
    "}\n",
    "}\n",
    "$$\n",
    "\n",
    "Using logarithm identities, this becomes:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\sum_{z}\n",
    "\\frac{1}{\\sigma_q \\sqrt{2\\pi}}\n",
    "\\; e^{\n",
    "-\\frac{1}{2}\n",
    "\\left(\\frac{x - \\mu_q}{\\sigma_q}\\right)^2\n",
    "}\n",
    "\\cdot\n",
    "\\left(\n",
    "\\log \\frac{\\sigma_p}{\\sigma_q}\n",
    "-\n",
    "\\frac{(x - \\mu_q)^2}{2\\sigma_q^2}\n",
    "+\n",
    "\\frac{(x - \\mu_p)^2}{2\\sigma_p^2}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "The normal distribution for $q$ in front has a constant factor, so we can pull it out:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{1}{\\sigma_q \\sqrt{2\\pi}}\n",
    "\\sum_{z}\n",
    "e^{-\\frac{1}{2}\\left(\\frac{x - \\mu_q}{\\sigma_q}\\right)^2}\n",
    "\\left(\n",
    "\\log \\frac{\\sigma_p}{\\sigma_q}\n",
    "-\n",
    "\\frac{(x - \\mu_q)^2}{2\\sigma_q^2}\n",
    "+\n",
    "\\frac{(x - \\mu_p)^2}{2\\sigma_p^2}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "What does this look like?\n",
    "\n",
    "We are weighting the expression\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "\\log \\frac{\\sigma_p}{\\sigma_q}\n",
    "-\n",
    "\\frac{(x - \\mu_q)^2}{2\\sigma_q^2}\n",
    "+\n",
    "\\frac{(x - \\mu_p)^2}{2\\sigma_p^2}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "by the Gaussian kernel for $q(x \\mid z)$.\n",
    "This is exactly the definition of an **expected value** under $q$.\n",
    "\n",
    "Since this entire expression is the KL divergence and our goal is to *minimize*\n",
    "the KL divergence, we can ignore the constant factor in front.\n",
    "(Minimizing $A\\,D_{KL}$ is equivalent to minimizing $D_{KL}$.)\n",
    "\n",
    "Thus we can rewrite the expression as:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\mathbb{E}_q\n",
    "\\left[\n",
    "\\log \\frac{\\sigma_p}{\\sigma_q}\n",
    "-\n",
    "\\frac{(x - \\mu_q)^2}{2\\sigma_q^2}\n",
    "+\n",
    "\\frac{(x - \\mu_p)^2}{2\\sigma_p^2}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Separating the expectation across terms:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\log \\frac{\\sigma_p}{\\sigma_q}\n",
    "-\n",
    "\\frac{1}{2\\sigma_q^2}\\,\n",
    "\\mathbb{E}_q\\!\\left[(x - \\mu_q)^2\\right]\n",
    "+\n",
    "\\frac{1}{2\\sigma_p^2}\\,\n",
    "\\mathbb{E}_q\\!\\left[(x - \\mu_p)^2\\right]\n",
    "$$\n",
    "\n",
    "Now the variance appears.\n",
    "Recall that for a random variable $x$:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(x) = \\sigma^2 = \\mathbb{E}\\!\\left[(x - \\mu)^2\\right]\n",
    "$$\n",
    "\n",
    "(?) Since $x \\sim q$, we can replace the expectation involving $\\mu_q$:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\log \\frac{\\sigma_p}{\\sigma_q}\n",
    "-\n",
    "\\frac{\\sigma_q^2}{2\\sigma_q^2}\n",
    "+\n",
    "\\frac{1}{2\\sigma_p^2}\n",
    "\\mathbb{E}_q\\!\\left[(x - \\mu_p)^2\\right]\n",
    "$$\n",
    "\n",
    "which simplifies to:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\log \\frac{\\sigma_p}{\\sigma_q}\n",
    "-\n",
    "\\frac{1}{2}\n",
    "+\n",
    "\\frac{1}{2\\sigma_p^2}\n",
    "\\mathbb{E}_q\\!\\left[(x - \\mu_p)^2\\right]\n",
    "$$\n",
    "\n",
    "Now we need to handle the remaining term\n",
    "$\\mathbb{E}_q[(x - \\mu_p)^2]$,\n",
    "which can be expanded using a simple algebraic identity before further\n",
    "simplification.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_q (x - \\mu_p)^2 \\\\\n",
    "= \\mathbb{E}_q \\big((x - \\mu_q) + (\\mu_q - \\mu_p)\\big)^2 \\\\\n",
    "= \\mathbb{E}_q \\Big(\n",
    "(x - \\mu_q)^2\n",
    "+ 2(x - \\mu_q)(\\mu_q - \\mu_p)\n",
    "+ (\\mu_q - \\mu_p)^2\n",
    "\\Big) \\\\\n",
    "= \\mathbb{E}_q[(x - \\mu_q)^2]\n",
    "+ 2\\,\\mathbb{E}_q[(x - \\mu_q)(\\mu_q - \\mu_p)]\n",
    "+ \\mathbb{E}_q[(\\mu_q - \\mu_p)^2]\n",
    "$$\n",
    "\n",
    "Again, we can replace our first expected value since it is the variance formula,\n",
    "and the last expected value is just a constant involving $(\\mu_q - \\mu_p)$\n",
    "(the expected value of a constant is just the constant; the only random variable\n",
    "here is $x$):\n",
    "\n",
    "$$\n",
    "= \\sigma_q^2\n",
    "+ 2\\,\\mathbb{E}_q[(x - \\mu_q)(\\mu_q - \\mu_p)]\n",
    "+ (\\mu_q - \\mu_p)^2 \\\\\n",
    "= \\sigma_q^2\n",
    "+ (\\mu_q - \\mu_p)^2\n",
    "+ 2(\\mu_q - \\mu_p)\\,\\mathbb{E}_q[x - \\mu_q]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma_q^2\n",
    "+ (\\mu_q - \\mu_p)^2\n",
    "+ 2(\\mu_q - \\mu_p)\\big(\\mathbb{E}_q[x] - \\mathbb{E}_q[\\mu_q]\\big) \\\\\n",
    "= \\sigma_q^2\n",
    "+ (\\mu_q - \\mu_p)^2\n",
    "+ 2(\\mu_q - \\mu_p)(\\mu_q - \\mu_q) \\\\\n",
    "= \\sigma_q^2 + (\\mu_q - \\mu_p)^2\n",
    "$$\n",
    "\n",
    "Let’s plug this result back into our original equation.\n",
    "\n",
    "$$\n",
    "\\log \\frac{\\sigma_p}{\\sigma_q}\n",
    "- \\frac{1}{2}\n",
    "+ \\frac{1}{2\\sigma_p^2}\\,\\mathbb{E}_q (x - \\mu_p)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\log \\frac{\\sigma_p}{\\sigma_q}\n",
    "- \\frac{1}{2}\n",
    "+ \\frac{\\sigma_q^2 + (\\mu_q - \\mu_p)^2}{2\\sigma_p^2}\n",
    "$$\n",
    "\n",
    "Now recall that the distribution $p(z)$ we want to map to is a **Standard Normal**\n",
    "distribution with $\\mu_p = 0$ and $\\sigma_p = 1$. Substituting these values gives:\n",
    "\n",
    "$$\n",
    "= \\log \\frac{1}{\\sigma_q}\n",
    "- \\frac{1}{2}\n",
    "+ \\frac{\\sigma_q^2 + \\mu_q^2}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2}\\log \\frac{1}{\\sigma_q^2}\n",
    "- \\frac{1}{2}\n",
    "+ \\frac{\\sigma_q^2 + \\mu_q^2}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2}\n",
    "\\left(\n",
    "\\log \\frac{1}{\\sigma_q^2}\n",
    "- 1\n",
    "+ \\sigma_q^2\n",
    "+ \\mu_q^2\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Remember this equation from before for our Evidence Lower Bound?\n",
    "\n",
    "$$\n",
    "\\sum_{z}\n",
    "q_\\theta(z \\mid x)\\cdot\n",
    "\\log \\frac{p_\\phi(x \\mid z)p(z)}{q_\\theta(z \\mid x)}\n",
    "\\;\\le\\;\n",
    "\\log p(x)\n",
    "$$\n",
    "\n",
    "We can now take our ELBO and split it up to form a loss function that includes\n",
    "both the **KL divergence** term and the **reconstruction loss**.\n",
    "\n",
    "$$\n",
    "\\sum_{z}\n",
    "q_\\theta(z \\mid x)\\cdot\n",
    "\\log \\frac{p_\\phi(x \\mid z)p(z)}{q_\\theta(z \\mid x)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\sum_{z}\n",
    "q_\\theta(z \\mid x)\\cdot\n",
    "\\log \\frac{p_\\phi(x \\mid z)p(z)}{q_\\theta(z \\mid x)}\n",
    "$$\n",
    "\n",
    "Splitting the logarithm:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\sum_{z}\n",
    "q_\\theta(z \\mid x)\\cdot\n",
    "\\log \\frac{p(z)}{q_\\theta(z \\mid x)}\n",
    "+\n",
    "\\sum_{z}\n",
    "q_\\theta(z \\mid x)\\cdot\n",
    "\\log p_\\phi(x \\mid z)\n",
    "$$\n",
    "\n",
    "Rewriting the first term and recognizing expectations:\n",
    "\n",
    "$$\n",
    "=\n",
    "-\n",
    "\\sum_{z}\n",
    "q_\\theta(z \\mid x)\\cdot\n",
    "\\log \\frac{q_\\theta(z \\mid x)}{p(z)}\n",
    "+\n",
    "\\mathbb{E}_q[\\log p_\\phi(x \\mid z)]\n",
    "$$\n",
    "\n",
    "The first term is exactly a KL divergence, so we obtain:\n",
    "\n",
    "$$\n",
    "=\n",
    "- D_{KL}\\big(q_\\theta(z \\mid x)\\,\\|\\,p(z)\\big)\n",
    "+\n",
    "\\mathbb{E}_q[\\log p_\\phi(x \\mid z)]\n",
    "$$\n",
    "\n",
    "or, more compactly:\n",
    "\n",
    "$$\n",
    "= - D_{KL} + \\text{Reconstruction}\n",
    "$$\n",
    "\n",
    "Therefore, the ELBO consists of **negative KL divergence** plus the **expected\n",
    "log-likelihood**, which is our reconstruction term. Since we want to maximize\n",
    "the ELBO, we now substitute the closed-form expression we derived earlier for\n",
    "the KL divergence between $q(z \\mid x)$ and a standard normal prior $p(z)$:\n",
    "\n",
    "$$\n",
    "=\n",
    "- D_{KL}\\big(q_\\theta(z \\mid x)\\,\\|\\,p(z)\\big)\n",
    "+\n",
    "\\mathbb{E}_q[\\log p_\\phi(x \\mid z)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "-\n",
    "\\frac{1}{2}\n",
    "\\left(\n",
    "\\log \\frac{1}{\\sigma_q^2}\n",
    "- 1\n",
    "+ \\sigma_q^2\n",
    "+ \\mu_q^2\n",
    "\\right)\n",
    "+\n",
    "\\mathbb{E}_q[\\log p_\\phi(x \\mid z)]\n",
    "$$\n",
    "\n",
    "Simplifying the negative sign:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{1}{2}\n",
    "\\left(\n",
    "\\log \\sigma_q^2\n",
    "+ 1\n",
    "- \\sigma_q^2\n",
    "- \\mu_q^2\n",
    "\\right)\n",
    "+\n",
    "\\mathbb{E}_q[\\log p_\\phi(x \\mid z)]\n",
    "$$\n",
    "\n",
    "That is the final function that we want to maximize, so we can force the\n",
    "Evidence Lower Bound (ELBO) to increase, and therefore cause $p(x)$ to also\n",
    "increase. However, in deep learning we typically perform **gradient descent**\n",
    "rather than ascent. Maximizing a function is therefore equivalent to minimizing\n",
    "the negative of that function.\n",
    "\n",
    "So our final loss for the model is:\n",
    "\n",
    "$$\n",
    "\\min_{\\theta, \\phi}\n",
    "\\;-\\frac{1}{2}\n",
    "\\left(\n",
    "\\log \\sigma_q^2\n",
    "+ 1\n",
    "- \\sigma_q^2\n",
    "- \\mu_q^2\n",
    "\\right)\n",
    "-\n",
    "\\mathbb{E}_q[\\log p_\\phi(x \\mid z)]\n",
    "$$\n",
    "\n",
    "And that’s it! (Almost)\n",
    "\n",
    "We have derived the final form of our loss function. Here are the components we\n",
    "need to compute it:\n",
    "\n",
    "- $\\sigma_q^2$:\n",
    "  Use a neural network parameterized by $\\theta$ to compute the **variance** of\n",
    "  a sample.\n",
    "\n",
    "- $\\mu_q^2$:\n",
    "  Use a neural network parameterized by $\\theta$ to compute the **mean** of a\n",
    "  sample.\n",
    "\n",
    "- $\\log p_\\phi(x \\mid z)$:\n",
    "  The **reconstruction loss**.\n",
    "  But there’s a problem — what is $z$?\n",
    "\n",
    "\n",
    "The variable $z$ is our latent variable (of some chosen dimensionality) that is\n",
    "**sampled** from our distribution. The model learns the parameters of a Gaussian\n",
    "(mean and variance), and then $z$ is a **random sample** drawn from the normal\n",
    "distribution defined by those predicted parameters.\n",
    "\n",
    "**Problem:** sampling introduces a stochastic node and breaks the computational graph.\n",
    "\n",
    "---\n",
    "\n",
    "### Reparamaterization Trick\n",
    "\n",
    "If you recall, backpropagation requires performing the chain rule through a\n",
    "sequence of nested functions. One of the requirements for this to work is that\n",
    "all the functions must be **deterministic**: given the same input, they must\n",
    "produce the same output. If a function is not deterministic, then its derivative\n",
    "cannot be computed.\n",
    "\n",
    "So how do we handle this?\n",
    "\n",
    "The latent variable $z$ is sampled from a Gaussian distribution whose parameters\n",
    "are predicted by $q(z \\mid x)$. If we directly and randomly sample $z$ from this\n",
    "distribution, there is no way for backpropagation to make its way back to\n",
    "previous nodes in the computational graph, as illustrated above.\n",
    "\n",
    "Instead, we make use of a neat property of **Normal distributions** that allows\n",
    "us to move the stochastic node out of the way.\n",
    "\n",
    "Let us say:\n",
    "\n",
    "$$\n",
    "X \\sim \\mathcal{N}(\\mu = A,\\; \\sigma^2 = B)\n",
    "$$\n",
    "\n",
    "Normal distributions have the following properties:\n",
    "\n",
    "- **Mean shift**  \n",
    "  $$\n",
    "  X + K \\sim \\mathcal{N}(\\mu = A + K,\\; \\sigma^2 = B)\n",
    "  $$\n",
    "  Adding a constant to a random variable shifts the mean of the distribution.\n",
    "\n",
    "- **Variance scaling**  \n",
    "  $$\n",
    "  X \\cdot K \\sim \\mathcal{N}(\\mu = A,\\; \\sigma^2 = B \\cdot K^2)\n",
    "  $$  \n",
    "  Multiplying by a constant scales the variance by the square of that constant.\n",
    "\n",
    "\n",
    "This means that if we want to sample from an arbitrary normal distribution\n",
    "$$\n",
    "\\mathcal{N}(\\mu = A,\\; \\sigma^2 = B),\n",
    "$$\n",
    "we can instead sample from a **standard normal distribution**\n",
    "$$\n",
    "\\mathcal{N}(\\mu = 0,\\; \\sigma^2 = 1)\n",
    "$$\n",
    "and then shift and scale it appropriately.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(\\mu = A,\\; \\sigma^2 = B)\n",
    "=\n",
    "A + \\sqrt{B} \\cdot \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "When we scale the distribution, if $B$ is the variance we want and we multiply\n",
    "by $\\sqrt{B}$, we are really just multiplying by $\\sigma$, the **standard\n",
    "deviation**, which is always the square root of the variance.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20251210162750705790/decoder_model.webp\" alt=\"alt\" width=\"600\">\n",
    "</p>\n",
    "\n",
    "Both directly sampling or the reparamaterization trick will give identical results, but by doing reparameterization, we split off the random nodes of the Neural Network away from the rest so our model remains fully differentiable! Lets quickly see how this works in practice:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf72467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propogated Gradient without Reparam. Trick tensor([0.])\n",
      "Propogated Gradient with Reparam. Trick tensor([32.5925])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "### Without Reparamaterization ###\n",
    "input_tensor = torch.tensor([3.])\n",
    "w1 = torch.tensor([2.], requires_grad=True)\n",
    "w_mu = torch.tensor([8.], requires_grad=True)\n",
    "w_sd = torch.tensor([9.], requires_grad=True)\n",
    "\n",
    "w1_out = w1*input_tensor\n",
    "mu = w_mu*w1_out\n",
    "std = w_sd*w1_out\n",
    "\n",
    "z = torch.normal(mean=mu, std=std)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(\"Propogated Gradient without Reparam. Trick\", w1.grad)\n",
    "\n",
    "### With Reparamaterization ###\n",
    "w1_out = w1*input_tensor\n",
    "mu = w_mu*w1_out\n",
    "std = w_sd*w1_out\n",
    "\n",
    "z = mu + std*torch.randn(1)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(\"Propogated Gradient with Reparam. Trick\", w1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c367da",
   "metadata": {},
   "source": [
    "### Log-Variance in the Reparameterization Trick\n",
    "\n",
    "This has nothing to do with theory, but rather **stability and general practice**.\n",
    "As we showed before, our neural network predicts the mean $\\mu$ and the variance\n",
    "$\\sigma^2$. However, for numerical stability, models typically predict the\n",
    "**log variance** instead of the variance directly.\n",
    "\n",
    "This choice is mostly practical, but it is widely used and convenient, so we\n",
    "will follow it here. This means we need to keep a few small details in mind.\n",
    "\n",
    "\n",
    "When applying the reparameterization trick, we need to multiply by the **standard\n",
    "deviation**, which is the square root of the variance.\n",
    "\n",
    "Since the network outputs $\\log \\sigma^2$, we compute $\\sigma$ as:\n",
    "\n",
    "$$\n",
    "e^{0.5 \\cdot \\log(\\sigma^2)}\n",
    "=\n",
    "e^{\\log(\\sqrt{\\sigma^2})}\n",
    "=\n",
    "e^{\\log \\sigma}\n",
    "=\n",
    "\\sigma\n",
    "$$\n",
    "\n",
    "This allows us to recover the correct scaling factor while keeping everything\n",
    "numerically stable and differentiable.\n",
    "\n",
    "Additionally, our loss function already contains log-variance terms, so using\n",
    "$\\log \\sigma^2$ directly ends up being slightly more convenient overall.\n",
    "\n",
    "---\n",
    "\n",
    "### Recap\n",
    "\n",
    "That was a lot of derivation! None of it is especially difficult, but it does\n",
    "require stepping through carefully to understand the **purpose** and\n",
    "**architecture** of the model.\n",
    "\n",
    "The main takeaway is that our final loss function is:\n",
    "\n",
    "$$\n",
    "\\min_{\\theta, \\phi}\n",
    "\\;\n",
    "-\\frac{1}{2}\n",
    "\\left(\n",
    "\\log \\sigma_q^2\n",
    "+ 1\n",
    "- \\sigma_q^2\n",
    "- \\mu_q^2\n",
    "\\right)\n",
    "-\n",
    "\\mathbb{E}_q[\\log p_\\phi(x \\mid z)]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mu_q$ is the mean predicted by the encoder network,\n",
    "- $\\log \\sigma_q^2$ is the log-variance predicted by the encoder,\n",
    "- and the expectation term is the **reconstruction loss** produced by the decoder.\n",
    "\n",
    "This completes the full VAE objective and sets the stage for an actual\n",
    "implementation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-zoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
